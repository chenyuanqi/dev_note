
### 大数据问题汇总
- 两个大文件中找出共同记录
> 给定 a、b 两个文件，各存放 50 亿个 url，每个 url 各占 64 字节，内存限制是 4G，让你找出 a、b 文件共同的 url？  
>
> 方案 1：“分治算法”  
> 1、遍历文件 a，对每个 url 求取 hash(url)%1000，然后根据所取得的值将 url 分别存储到 1000 个小文件（记为 a0,a1,...,a999 每个小文件约 300M），为什么是 1000 呢？主要根据内存大小和要分治的文件大小来计算，我们就大致可以把 320G 大小分为 1000 份，每份大约 300M（当然，到底能不能分布尽量均匀，得看 hash 函数的设计）  
> 2、遍历文件 b，采取和 a 相同的方式将 url 分别存储到 1000 个小文件（记为 b0,b1,...,b999 ）  
> 为什么要这样做？文件 a 的 hash 映射和文件 b 的 hash 映射函数要保持一致，这样的话相同的 url 就会保存在对应的小文件中，比如，如果 a 中有一个 url 记录 data1 被 hash 到了 a99 文件中，那么如果 b 中也有相同 url，则一定被 hash 到了 b99 中。  
> 所以现在问题转换成了：找出 1000 对小文件中每一对相同的 url（不对应的小文件不可能有相同的 url）  
> 3、求每对小文件中相同的 url 时，可以把其中一个小文件的 url 存储到 hash_set 中。然后遍历另一个小文件的每个 url，看其是否在刚才构建的 hash_set 中，如果是，那么就是共同的 url，存到文件里面就可以了。  
> 
> 方案 2：
> 如果允许有一定的错误率，可以使用 Bloom filter，4G 内存大概可以表示 340 亿 bit。将其中一个文件中的 url 使用 Bloom filter 映射为这 340 亿 bit，然后挨个读取另外一个文件的 url，检查是否与 Bloom filter，如果是，那么该 ur l 应该是共同的 url（注意会有一定的错误率）  

- 10g 文件，用 php 查看它的行数
> 通常大文件大家用 while 来循环的逐行统计，这样的效率太慢。  
> 最快的方法是多行统计，每次读取 N 个字节，然后再统计行数，这样比逐行效率高多了。  
```php
function count_line($file){
    $fp=fopen($file, "r");
    $i=0;
    while(!feof($fp)) {
        //每次读取 2M
        if($data=fread($fp,1024*1024*2)){
            //计算读取到的行数
            $num=substr_count($data,"\n");
            $i+=$num;
        }
    }
    fclose($fp);
    return $i;
}
```

- 海量日志数据，提取出某日访问百度次数最多的那个 IP
> 分而治之 + Hash  
> 1.IP 地址最多有 2^32=4G 种取值情况，所以不能完全加载到内存中处理；   
> 2.可以考虑采用 “分而治之” 的思想，按照 IP 地址的 Hash(IP)%1024 值，把海量 IP 日志分别存储到 1024 个小文件中。这样，每个小文件最多包含 4MB 个 IP 地址；   
> 3.对于每一个小文件，可以构建一个 IP 为 key，出现次数为 value 的 Hash map，同时记录当前出现次数最多的那个 IP 地址；  
> 4.可以得到 1024 个小文件中的出现次数最多的 IP，再依据常规的排序算法得到总体上出现次数最多的 IP；

- 有 10 亿条订单数据，属于 1000 个司机的，请取出订单量前 20 的司机
> 不要用常用思路来处理，10 亿数据 你再怎么优化，全表求和，都是要死人的。  
> 我们从设计上解决这个问题。只有一千个司机。我们可以做个简单哈希，分库分表，% 求余数。保证这一千个司机分在一千个表里，每个人有每个人的单独表。引擎用 MYSAIM，求表中数据的总数，效率飞快，遍历一千张表，求最大前二十即可。

- 有一个 1G 大小的一个文件，里面每一行是一个词，词的大小不超过 16 字节，内存限制大小是 1M。返回频数最高的 100 个词。 
> 顺序读文件中，对于每个词 x，取 hash(x)%5000，然后按照该值存到 5000 个小文件（记为 x0,x1,...x4999）中。这样每个文件大概是 200k 左右。  
> 如果其中的有的文件超过了 1M 大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过 1M。  
> 对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用 trie 树 / hash_map 等），并取出出现频率最大的 100 个词（可以用含 100 个结点的最小堆），并把 100 个词及相应的频率存入文件，这样又得到了 5000 个文件。下一步就是把这 5000 个文件进行归并（类似与归并排序）的过程了。  

- 在 2.5 亿个整数中找出不重复的整数，注，内存不足以容纳这 2.5 亿个整数。
> 采用 2-Bitmap（每个数分配 2bit，00 表示不存在，01 表示出现一次，10 表示多次，11 无意义）进行，共需内存 2^32 * 2 bit=1 GB 内存，还可以接受。然后扫描这 2.5 亿个整数，查看 Bitmap 中相对应位，如果是 00 变 01，01 变 10，10 保持不变。所描完事后，查看 bitmap，把对应位是 01 的整数输出即可。




